{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompLing Final Project- Automated Web Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we will learn a concept of the package-BeautifulSoup and its implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/outline.png\" alt=\"Outline\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/http.png\" alt=\"Http\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP functions as a request-response protocol in the client-server computing model. The **client** submits an HTTP _request_ message to the server. The **server**, which provides resources such as HTML files and other contents, returns a _response_ message to the client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the titles and comments from the website-Reddit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h3 class=\"_eYtD2XCVieq6emjKBH3m\">What is something in your search history that you are ashamed off and it's not porn or hentai of any kind?</h3>, <h3 class=\"_eYtD2XCVieq6emjKBH3m\">AITA for refusing to support my son's pregnant girlfriend until she takes a paternity test?</h3>, <h3 class=\"_eYtD2XCVieq6emjKBH3m\">me_irl</h3>, <h3 class=\"_eYtD2XCVieq6emjKBH3m\">My Valkyrie Cosplay from God of War</h3>, <h3 class=\"_eYtD2XCVieq6emjKBH3m\">Runner who slapped reporter's behind on live TV has been identified and banned from future running events, charges pending</h3>, <h3 class=\"_eYtD2XCVieq6emjKBH3m\">US States ordered by life-expectancy - Animation [OC]</h3>, <h3 class=\"_eYtD2XCVieq6emjKBH3m\">Forum - Development Manifesto - Development Manifesto: Balance in Path of Exile: Conquerors of the Atlas - Path of Exile</h3>]\n",
      "[<span class=\"FHCV02u6Cp2zYL0fhQPsO\">8.7k comments</span>, <span class=\"FHCV02u6Cp2zYL0fhQPsO\">5.4k comments</span>, <span class=\"FHCV02u6Cp2zYL0fhQPsO\">216 comments</span>, <span class=\"FHCV02u6Cp2zYL0fhQPsO\">387 comments</span>, <span class=\"FHCV02u6Cp2zYL0fhQPsO\">3.2k comments</span>, <span class=\"FHCV02u6Cp2zYL0fhQPsO\">2.8k comments</span>, <span class=\"FHCV02u6Cp2zYL0fhQPsO\">2.7k comments</span>]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "'''url=the url of the website that I want to scrape,\n",
    "html=use the urlopen function to fetch URLs\n",
    "soup=use the beautifulsoup to parse the response from the server'''\n",
    "\n",
    "url = \"https://www.reddit.com/\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "reddit_text = soup.find_all(\"h3\",class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "reddit_texts = [e.text for e in reddit_text]\n",
    "\n",
    "reddit_comment = soup.find_all(\"span\",class_= \"FHCV02u6Cp2zYL0fhQPsO\")\n",
    "reddit_comments = [f.text for f in reddit_comment]\n",
    "\n",
    "print(reddit_text)\n",
    "print(reddit_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What is something in your search history that you are ashamed off and it's not porn or hentai of any kind?\", \"AITA for refusing to support my son's pregnant girlfriend until she takes a paternity test?\", 'me_irl', 'My Valkyrie Cosplay from God of War', \"Runner who slapped reporter's behind on live TV has been identified and banned from future running events, charges pending\", 'US States ordered by life-expectancy - Animation [OC]', 'Forum - Development Manifesto - Development Manifesto: Balance in Path of Exile: Conquerors of the Atlas - Path of Exile']\n",
      "['8.7k comments', '5.4k comments', '216 comments', '387 comments', '3.2k comments', '2.8k comments', '2.7k comments']\n"
     ]
    }
   ],
   "source": [
    "print(reddit_texts)\n",
    "print(reddit_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind\", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test\", 'meirl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events charges pending\", 'us states ordered by lifeexpectancy  animation oc', 'forum  development manifesto  development manifesto balance in path of exile conquerors of the atlas  path of exile']\n"
     ]
    }
   ],
   "source": [
    " '''clear all the punctuations and lowercase the texts in the title'''\n",
    "    \n",
    "a=0\n",
    "for a in range(len(reddit_texts)):\n",
    "    for b in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_‘{|}~':\n",
    "        reddit_texts[a] = reddit_texts[a].replace(b, \"\")\n",
    "        reddit_texts[a] = reddit_texts[a].lower()\n",
    "\n",
    "print(reddit_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "'''check if there is a repeated title'''\n",
    "print(len(reddit_texts))\n",
    "print(len(reddit_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind\", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test\", 'meirl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events charges pending\", 'us states ordered by lifeexpectancy  animation oc', 'forum  development manifesto  development manifesto balance in path of exile conquerors of the atlas  path of exile']\n"
     ]
    }
   ],
   "source": [
    "'''delete the repeated title if there is one'''\n",
    "c=0\n",
    "reddit_texts1=[]\n",
    "for c in range(len(reddit_texts)):\n",
    "    if reddit_texts[c] not in reddit_texts1:\n",
    "        reddit_texts1.append(reddit_texts[c])\n",
    "\n",
    "print(reddit_texts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "'''check again'''\n",
    "print(len(reddit_texts1))\n",
    "print(len(reddit_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8.7k ', '5.4k ', '216 ', '387 ', '3.2k ', '2.8k ', '2.7k ']\n"
     ]
    }
   ],
   "source": [
    "'''delete the word \"comments\"'''\n",
    "d=0\n",
    "for d in range(len(reddit_comments)):\n",
    "    reddit_comments[d]=reddit_comments[d].replace(\"comments\",\"\")\n",
    "    \n",
    "print(reddit_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind\", 'forum  development manifesto  development manifesto balance in path of exile conquerors of the atlas  path of exile']\n",
      "['8.7k ', '2.7k ']\n"
     ]
    }
   ],
   "source": [
    "'''search word from the titles, if it matches, append the titles that contain the search word into the \n",
    "new list reddit_search, and also append the corresponding comments to the new list reddit_comments'''\n",
    "    \n",
    "search_word=[\"history\",\"development\"]\n",
    "e=0\n",
    "reddit_search=[]\n",
    "reddit_search_comment=[]\n",
    "for e in range(len(reddit_texts1)):\n",
    "    words = reddit_texts1[e].split()\n",
    "    for word in words:\n",
    "        if word in search_word and reddit_texts1[e] not in reddit_search:\n",
    "            reddit_search.append(reddit_texts1[e])\n",
    "            reddit_search_comment.append(reddit_comments[e])\n",
    "    \n",
    "print(reddit_search)\n",
    "print(reddit_search_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind\", 'forum  development manifesto  development manifesto balance in path of exile conquerors of the atlas  path of exile']\n",
      "[870.0, 270.0]\n"
     ]
    }
   ],
   "source": [
    "'''delete the \".\",\"k\",\"\", plus 2 zeros and convert the list to interger\n",
    "if the number is too large, the format will become BLOB when storing in the database which is hard to read\n",
    "so I divide the number by 10 to get the numeric format'''\n",
    "\n",
    "f=0\n",
    "for f in range(len(reddit_search_comment)):\n",
    "    if \"k\" in reddit_search_comment[f]:\n",
    "        reddit_search_comment[f]=reddit_search_comment[f].replace(\".\",\"\")\n",
    "        reddit_search_comment[f]=reddit_search_comment[f].replace(\"k\",\"\")\n",
    "        reddit_search_comment[f]=reddit_search_comment[f].replace(\" \",\"\")\n",
    "        reddit_search_comment[f]=reddit_search_comment[f]+\"00\"\n",
    "        reddit_search_comment[f]=(int(reddit_search_comment[f])/10)\n",
    "    else:\n",
    "        reddit_search_comment[f]=(int(reddit_search_comment[f])/10)\n",
    "        \n",
    "        \n",
    "print(reddit_search)\n",
    "print(reddit_search_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_search</th>\n",
       "      <th>reddit_search_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is something in your search history that ...</td>\n",
       "      <td>870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forum  development manifesto  development mani...</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       reddit_search  reddit_search_comment\n",
       "0  what is something in your search history that ...                  870.0\n",
       "1  forum  development manifesto  development mani...                  270.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''use the panda package to convert to dataframe(table) format'''\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"reddit_search\":reddit_search, \"reddit_search_comment\":reddit_search_comment\n",
    "    })\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A new file called test.db is created where the database will be stored,\n",
    "    connect Python to database, create the table in database'''\n",
    "\n",
    "conn = sqlite3.connect('test.db')\n",
    "c = conn.cursor()\n",
    "#c.execute(\"CREATE TABLE COMPLING1206 (TITLE TEXT, COMMENTS INTEGER);\"\n",
    "    #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cursor1.png\" alt=\"cursor\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''insert the data into the table'''\n",
    "\n",
    "g=0\n",
    "for g in range(len(df)):\n",
    "    c.execute(\"INSERT INTO COMPLING1206 VALUES (?,?)\", (df[\"reddit_search\"][g],df[\"reddit_search_comment\"][g],))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import URLError\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "def redditsqlitegogo():\n",
    "    try:\n",
    "        url = \"https://www.reddit.com/\"\n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        reddit_text = soup.find_all(\"h3\",class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "        reddit_texts = [e.text for e in reddit_text]\n",
    "\n",
    "        reddit_comment = soup.find_all(\"span\",class_= \"FHCV02u6Cp2zYL0fhQPsO\")\n",
    "        reddit_comments = [f.text for f in reddit_comment]\n",
    "    \n",
    "        #print(reddit_comments)\n",
    "\n",
    "       \n",
    "        a=0\n",
    "        for a in range(len(reddit_texts)):\n",
    "            for b in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_‘{|}~':\n",
    "                reddit_texts[a] = reddit_texts[a].replace(b, \" \")\n",
    "                reddit_texts[a] = reddit_texts[a].lower()\n",
    "    \n",
    "        #print(reddit_texts)   \n",
    "        \n",
    "        c=0\n",
    "        reddit_texts1=[]\n",
    "        for c in range(len(reddit_texts)):\n",
    "            if reddit_texts[c] not in reddit_texts1:\n",
    "                reddit_texts1.append(reddit_texts[c])\n",
    "    \n",
    "        print(reddit_texts)\n",
    "        # When encountering more than two repeated titles, it's hard to match the titles and comments,\n",
    "        #so I abandon the repeated titles for this time period (this happens rarely)\n",
    "        h=0\n",
    "        if len(reddit_texts1) != len(reddit_comments):\n",
    "            reddit_texts1=[]\n",
    "            while h < len(reddit_comments):\n",
    "                reddit_texts1.append(\"No Value\")\n",
    "                h=h+1\n",
    "        \n",
    "        d=0\n",
    "        for d in range(len(reddit_comments)):\n",
    "            reddit_comments[d]=reddit_comments[d].replace(\"comments\",\"\")\n",
    "        \n",
    "        e=0\n",
    "        for e in range(len(reddit_comments)):\n",
    "            if \"k\" in reddit_comments[e]:\n",
    "                reddit_comments[e]=reddit_comments[e].replace(\".\",\"\")\n",
    "                reddit_comments[e]=reddit_comments[e].replace(\"k\",\"\")\n",
    "                reddit_comments[e]=reddit_comments[e].replace(\" \",\"\")\n",
    "                reddit_comments[e]=reddit_comments[e]+\"000\"\n",
    "                reddit_comments[e]=(int(reddit_comments[e])/100)\n",
    "            else:\n",
    "                reddit_comments[e]=(int(reddit_comments[e])/10)\n",
    "            \n",
    "       \n",
    "        df = pd.DataFrame(\n",
    "            {\"reddit_texts1\":reddit_texts1, \"reddit_comments\":reddit_comments\n",
    "        })\n",
    "        df.head()\n",
    "\n",
    "        \n",
    "        conn = sqlite3.connect('test.db')\n",
    "        c = conn.cursor()\n",
    "        #c.execute(\"CREATE TABLE REDDITTILECOMMENTS11145 (TITLE TEXT, COMMENTS INTEGER);\"\n",
    "        #)\n",
    "\n",
    "\n",
    "\n",
    "        f=0\n",
    "        for f in range(len(df)):\n",
    "            c.execute(\"INSERT INTO COMPLING1206 VALUES (?,?)\", (df[\"reddit_texts1\"][f],df[\"reddit_comments\"][f],))\n",
    "            conn.commit()\n",
    "            \n",
    "    except URLError:\n",
    "        print(\"Be patient~let's try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be patient~let's try again\n",
      "Be patient~let's try again\n",
      "Be patient~let's try again\n",
      "Be patient~let's try again\n",
      "Be patient~let's try again\n",
      "Be patient~let's try again\n",
      "Be patient~let's try again\n",
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind \", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test \", 'me irl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events  charges pending\", 'us states ordered by life expectancy   animation  oc ', 'forum   development manifesto   development manifesto  balance in path of exile  conquerors of the atlas   path of exile']\n",
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind \", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test \", 'me irl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events  charges pending\", 'us states ordered by life expectancy   animation  oc ', 'forum   development manifesto   development manifesto  balance in path of exile  conquerors of the atlas   path of exile']\n",
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind \", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test \", 'me irl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events  charges pending\", 'us states ordered by life expectancy   animation  oc ', 'forum   development manifesto   development manifesto  balance in path of exile  conquerors of the atlas   path of exile']\n",
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind \", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test \", 'me irl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events  charges pending\", 'us states ordered by life expectancy   animation  oc ', 'forum   development manifesto   development manifesto  balance in path of exile  conquerors of the atlas   path of exile']\n",
      "Be patient~let's try again\n",
      "Be patient~let's try again\n",
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind \", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test \", 'me irl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events  charges pending\", 'us states ordered by life expectancy   animation  oc ', 'forum   development manifesto   development manifesto  balance in path of exile  conquerors of the atlas   path of exile']\n",
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind \", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test \", 'me irl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events  charges pending\", 'us states ordered by life expectancy   animation  oc ', 'forum   development manifesto   development manifesto  balance in path of exile  conquerors of the atlas   path of exile']\n",
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind \", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test \", 'me irl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events  charges pending\", 'us states ordered by life expectancy   animation  oc ', 'forum   development manifesto   development manifesto  balance in path of exile  conquerors of the atlas   path of exile']\n",
      "[\"what is something in your search history that you are ashamed off and it's not porn or hentai of any kind \", \"aita for refusing to support my son's pregnant girlfriend until she takes a paternity test \", 'me irl', 'my valkyrie cosplay from god of war', \"runner who slapped reporter's behind on live tv has been identified and banned from future running events  charges pending\", 'us states ordered by life expectancy   animation  oc ', 'forum   development manifesto   development manifesto  balance in path of exile  conquerors of the atlas   path of exile']\n",
      "Be patient~let's try again\n",
      "Be patient~let's try again\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d1b7d91de76a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "schedule.every(10).seconds.do(redditsqlitegogo)\n",
    "\n",
    "while 1:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At the Google Arts & Culture Lab in Paris, we’re all about exploring the relationship between art and technology. Since 2012, we’ve worked with artists and creators from many fields, developing experiments that let you design patterns in augmented reality, co-create poetry, or experience multisensory art installations. Today we’re launching two experiments to test the potential of artificial intelligence in the worlds of contemporary dance and fashion.For our first experiment, Runway Palette, we came together with The Business of Fashion, whose collection includes 140,000 photos of runway looks from almost 4,000 fashion shows. If you could attend one fashion show per day, it would take you more than ten years to see them all. By extracting the main colors of each look, we used machine learning to organize the images by color palette, resulting in an interactive visualization of four years of fashion by almost 1,000 designers.Everyone can now use the color palette visualization to explore colors, designers, seasons, and trends that come from Fashion Weeks worldwide.\\xa0 You can even snap or upload a picture of, let’s say, your closet, or autumn leaves, and discover how designers used a similar color palette in fashion.', 'Explore four years of runway looks organized by color using machine learning.', 'Take a picture or upload a photo to see how designers used a similar color palette in fashion. ', 'Click on a palette to see which runway looks have similar colors, and compare by collection.', 'For our second experiment, Living Archive, we continued our collaboration with Wayne McGregor to create an AI-driven choreography tool. Trained on over 100 hours of dance performances from Wayne’s 25-year archive, the experiment uses machine learning to predict and generate movement in the style of Wayne’s dancers. In July of this year, they used the tool in his creative process for a new work that premiered at the LA Music Center.\\xa0Today, we are making this experiment available to everyone. Living Archive lets you explore almost half a million poses from Wayne’s extensive archive, organized by visual similarity. Use the experiment to make connections between poses, or capture\\xa0 your own movement to create your very own choreography.', 'You can try our new experiments on the Google Arts & Culture experiments page or via our free app for iOS and Android.']\n"
     ]
    }
   ],
   "source": [
    "'''scrape articles from GoogleBlog,'''\n",
    "'''fetch google blog article, enter the next page and fetch the content of the article'''\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "url = \"https://www.blog.google/outreach-initiatives/arts-culture/\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "'''scrape the title of the next page (because the connection between the main page's url and the next page's url\n",
    "is main page's url + names of the titles of the next page)'''\n",
    "\n",
    "google_text = soup.find_all(\"h4\",class_=\"uni-blog-nup__header h-has-bottom-margin h-u-font-weight-medium\")\n",
    "google_texts = [e.text for e in google_text]\n",
    "\n",
    "'''lowercase the title of the article and join the \"-\" to become the part that comes at the very end of a URL'''\n",
    "a=0\n",
    "google_url=[]\n",
    "for a in range(len(google_texts)):\n",
    "    google_texts[a]=google_texts[a].lower()\n",
    "    url=google_texts[a].split()\n",
    "    google_url.append(\"-\".join(url))\n",
    "    \n",
    "\n",
    "'''connect the main page's url and the next page'''\n",
    "\n",
    "web=\"https://www.blog.google/outreach-initiatives/arts-culture/\"\n",
    "b=0\n",
    "total_url=[]\n",
    "for b in range(len(google_url)):\n",
    "    total_url.append(web+google_url[b]+\"/\")\n",
    "    \n",
    "    \n",
    "'''enter the next page to fetch the content of the article'''  \n",
    "\n",
    "url = total_url[0]\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "google_new=soup.find_all(\"div\",class_=\"rich-text\")\n",
    "google_news=[f.text for f in google_new]\n",
    "\n",
    "print(google_news) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump impeachment: House Democrats to unveil formal chargeshttps://bbc.in/2YygkMi\\xa0', 'Floods and power cuts hit South Africahttps://bbc.in/2LDYWR0\\xa0', 'Algeria jails two former prime ministers over corruptionhttps://bbc.in/35bKuY8\\xa0', 'France protest: Mass rallies called on sixth day of disruptionhttps://bbc.in/356qjLe\\xa0', 'New Zealand volcano: What we know about those affectedhttps://bbc.in/38roJFI\\xa0', \"Roxette singer Marie Fredriksson, who achieved global success in the 1990s with hits including 'It Must Have Been Love', dies aged 61http://bbc.in/2RDEIuo\\xa0\", 'Roxette singer Marie Fredriksson dies, aged 61https://bbc.in/348Pc7U\\xa0', 'Ice Bucket Challenge inspiration Pete Frates dies at 34https://bbc.in/36iNjqD\\xa0', \"Conditions at the Sydney Cricket Ground were described as  toxic  as play continued despite smoke from bushfires. For someone like me who smokes 40 a day, it's now like smoking 80 cigarettes a day.  https://bbc.in/36lVWAJ\\xa0bbccricketpic.twitter.com/CmKJHtCsen\", \"A plane has been caught on camera spinning out of control in an emergency landing in Costa RicaRemarkably no one was injured after the plane's tyres burst during landinghttp://bbc.in/2sg5I8Y\\xa0pic.twitter.com/0TnldZnTFs\", 'George Laurer, co-inventor of the barcode, dies at 94 https://bbc.in/2Pz9sud\\xa0pic.twitter.com/KA1Xb7Umhs', ' We have been protecting our forests. We have kept many oil companies away https://bbc.in/2LH15LI\\xa0', \"Authorities in Bangalore hope that drivers mistake traffic 'control' mannequins for real police officershttps://bbc.in/2E3zPDe\\xa0\", 'Gunman on run after deadly Czech hospital shootinghttps://bbc.in/2Rz8Yqt\\xa0', \"Nobel Peace Prize winner Aung San Suu Kyi used to be seen as a symbol of human rights, and spent years under house arrest for promoting democracyNow, as Myanmar's civilian leader, she is defending the very people who had previously imprisoned herhttp://bbc.in/2E5fnl6\\xa0pic.twitter.com/6YbQvfx4u0\", 'Myanmar Rohingya: Suu Kyi to defend genocide charge at UN courthttps://bbc.in/358UeCq\\xa0', 'Guernsey GPs to trial issuing digital medical certificateshttps://bbc.in/2sXWC0F\\xa0', \"Sydney smoke: Residents 'choking' on intense bushfire pollutionhttps://bbc.in/2Yx0P7r\\xa0\", 'NintendoSwitch teams up with Tencent and launches in China today but for under 18s gaming is off limits between 10pm and 8am weekdays. Is this a good idea? Should other countries crack down on late night gaming? Get in touch BBCTheBriefingpic.twitter.com/uyF5xCLjWH', \"Chilean military plane 'disappears' with 38 aboardhttps://bbc.in/2RDmrgS\\xa0\"]\n",
      "['Trump impeachment: House Democrats to unveil formal charges', 'Floods and power cuts hit South Africa', 'Algeria jails two former prime ministers over corruption', 'France protest: Mass rallies called on sixth day of disruption', 'New Zealand volcano: What we know about those affected', 'Roxette singer Marie Fredriksson dies, aged 61', 'Ice Bucket Challenge inspiration Pete Frates dies at 34', \"Conditions at the Sydney Cricket Ground were described as  toxic  as play continued despite smoke from bushfires. For someone like me who smokes 40 a day, it's now like smoking 80 cigarettes a day.  \", 'George Laurer, co-inventor of the barcode, dies at 94 ', ' We have been protecting our forests. We have kept many oil companies away ', \"Authorities in Bangalore hope that drivers mistake traffic 'control' mannequins for real police officers\", 'Gunman on run after deadly Czech hospital shooting', 'Myanmar Rohingya: Suu Kyi to defend genocide charge at UN court', 'Guernsey GPs to trial issuing digital medical certificates', \"Sydney smoke: Residents 'choking' on intense bushfire pollution\", \"Chilean military plane 'disappears' with 38 aboard\"]\n"
     ]
    }
   ],
   "source": [
    "'''enter the next page to fetch the content of the article'''  \n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "url = \"https://twitter.com/BBCWorld\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "tweet_text = soup.find_all(\"p\",class_=\"tweet-text\")\n",
    "tweet_texts = [e.text for e in tweet_text]\n",
    "\n",
    "'''the titles that I scraped have the url and some symbols after the titles, so here I delete them'''\n",
    "\n",
    "new_tweet=[]\n",
    "new_tweet2=[]\n",
    "a=0\n",
    "for a in range(len(tweet_texts)):\n",
    "    tweet_texts[a] = tweet_texts[a].replace('\"', \" \")\n",
    "    tweet_texts[a] = tweet_texts[a].replace(\"\\n\",\"\")\n",
    "    tweet_texts[a] = tweet_texts[a].replace(\"#\",\"\")\n",
    "    new_tweet = re.split(\"https://\",tweet_texts[a])\n",
    "    del new_tweet[len(new_tweet)-1]\n",
    "    new_tweet2.extend(new_tweet)\n",
    "    \n",
    "        \n",
    "print(tweet_texts)\n",
    "print(new_tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"NZ volcano rescuer tells of 'Chernobyl'-like scene\", \"NZ volcano rescuer tells of 'Chernobyl'-like scene\", 'Democrats set to unveil Trump impeachment charges', 'Antarctica-bound plane missing with 38 on board', 'Do apostrophes still matter?', \"Canadians held for a year by China are 'resilient'\", 'Video 2 minutes 10 secondsThe US city giving away free money', 'Gunman kills six in Czech hospital waiting room', 'Suu Kyi to defend Myanmar at UN genocide hearing', 'Ice Bucket Challenge inspiration dies at 34', 'Roxette singer Marie Fredriksson dies, aged 61', 'Ice Bucket Challenge inspiration dies at 34', 'Roxette singer Marie Fredriksson dies, aged 61', \"Sydney 'choking' on thick smoke from bushfires\", \"Drugs and guns found on Juice Wrld's jet\", 'Floods and power cuts hit South Africa', 'VideoOn the campaign trail with Biden', 'BBC World News TV', 'AudioBBC World Service Radio', 'VideoPlane spins on runway during emergency landing', \"Dummies in police uniforms 'control' city traffic\", 'VideoUS man arrested after wheelchair theft attempt', 'In pictures: Aftermath of deadly New Zealand eruption', \"The secrets of 'food porn' viral videos\", 'VideoThe US city giving away free money', 'VideoPlane spins on runway during emergency landing', 'VideoThe moment after eruption hit NZ tourist site', 'VideoHow a peace icon ended up at a genocide trial', 'VideoOn the campaign trail with Biden', 'New Zealand volcano: Who are the victims?', \"Why can't this doctor work in the UK?\", \"'I slept in a cupboard for three months'\", \"The rising star set to be the world's youngest PM\", 'Does UK election provide clues to Trump’s fortunes?', \"Are India's billion-dollar start-ups too powerful?\", \"Boris Johnson's race for trade deal strengthens EU hand\", '300 seconds at London Bridge', \"Camila Cabello: 'An absolute force of nature'\", \"'How I became a secret daytime DJ'\", '‘A common bug could have killed my baby’', \"The queue that claimed this woman's life\", 'Roxette singer Marie Fredriksson dies, aged 61', \"NZ volcano rescuer tells of 'Chernobyl'-like scene\", 'Antarctica-bound plane missing with 38 on board', 'Gunman kills six in Czech hospital waiting room', \"Drugs and guns found on Juice Wrld's jet\", 'Scar of Bikini A-bomb test still visible', 'Ice Bucket Challenge inspiration dies at 34', 'Surgeons withdraw support for heart disease advice', 'Does UK election provide clues to Trump’s fortunes?', \"Sydney 'choking' on thick smoke from bushfires\", 'What if all pregnancies were planned?', 'Is your sex life sustainable?', 'The wind-battered edge of the UK', 'The key books of the 21st Century?', 'Do you have many doppelgangers?', 'How TV can be good for children', 'The science of giving good gifts', \"Klopp prepared for Salzburg 'final' as Liverpool seek to reach last 16\", 'Formula 1 teams reject 2020 Pirelli tyres', \"'If you break the rules unintentionally, it's not cheating' - Reed dismisses accusations\", \"'A celebration for our nation' - Pakistan prepares to host first Test in a decade\", \"Lampard says Chelsea youngsters can 'make a mark' against Lille\", 'Russia handed four-year ban by Wada', \"Newcastle offer free half-season tickets to help fill St James' Park\"]\n"
     ]
    }
   ],
   "source": [
    "'''Scrape titles from BBC news'''\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "url = \"http://www.bbc.co.uk/news\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "bbc_text = soup.find_all(\"a\",class_=\"gs-c-promo-heading\")\n",
    "bbc_texts = [e.text for e in bbc_text]\n",
    "print(bbc_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>英文短名稱</th>\n",
       "      <th>二位代碼</th>\n",
       "      <th>三位代碼</th>\n",
       "      <th>數字代碼</th>\n",
       "      <th>ISO 3166-2</th>\n",
       "      <th>中文名稱</th>\n",
       "      <th>獨立主權</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>004</td>\n",
       "      <td>ISO 3166-2:AF</td>\n",
       "      <td>阿富汗</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>AX</td>\n",
       "      <td>ALA</td>\n",
       "      <td>248</td>\n",
       "      <td>ISO 3166-2:AX</td>\n",
       "      <td>奧蘭</td>\n",
       "      <td>否</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>008</td>\n",
       "      <td>ISO 3166-2:AL</td>\n",
       "      <td>阿爾巴尼亞</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "      <td>012</td>\n",
       "      <td>ISO 3166-2:DZ</td>\n",
       "      <td>阿爾及利亞</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>AS</td>\n",
       "      <td>ASM</td>\n",
       "      <td>016</td>\n",
       "      <td>ISO 3166-2:AS</td>\n",
       "      <td>美屬薩摩亞</td>\n",
       "      <td>否</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            英文短名稱 二位代碼 三位代碼 數字代碼     ISO 3166-2   中文名稱 獨立主權\n",
       "0     Afghanistan   AF  AFG  004  ISO 3166-2:AF    阿富汗    是\n",
       "1   Åland Islands   AX  ALA  248  ISO 3166-2:AX     奧蘭    否\n",
       "2         Albania   AL  ALB  008  ISO 3166-2:AL  阿爾巴尼亞    是\n",
       "3         Algeria   DZ  DZA  012  ISO 3166-2:DZ  阿爾及利亞    是\n",
       "4  American Samoa   AS  ASM  016  ISO 3166-2:AS  美屬薩摩亞    否"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "'''Scrape a table(countries) from the wikipedia'''\n",
    "\n",
    "url = \"https://zh.wikipedia.org/zh-tw/ISO_3166-1\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "\n",
    "tableheader = [th.text.replace(\"\\n\",\"\") for th in table.find_all(\"th\")]\n",
    "\n",
    "'''find the data of each country'''\n",
    "\n",
    "trs = table.find_all(\"tr\")[1:]\n",
    "rows = []\n",
    "for tr in trs:\n",
    "    rows.append([td.text.replace(\"\\n\",\"\").replace(\"\\xa0\",\"\") for td in tr.find_all('td')])\n",
    "\n",
    "#print(trs)\n",
    "#print(rows)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=rows, columns=tableheader)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
